name: "mlp"
hidden_dim: 512
num_layers: 3
dropout: 0.5
activation: "ReLU"
normalization: "BatchNorm"

